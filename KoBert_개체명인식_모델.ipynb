{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1-dofkcsMRX_1aXoltNB2LVLLcVaCuGGN","authorship_tag":"ABX9TyMt7qTriFaneqjs3sSd1VXm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kNcm66lhjP2","executionInfo":{"status":"ok","timestamp":1720432063861,"user_tz":-540,"elapsed":30961,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"7d56be40-69b0-4d4e-c6b0-2c251b9dc021"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-atdyr_46/kobert-tokenizer_817765ed7c164b9fa6068d82fab159ad\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-atdyr_46/kobert-tokenizer_817765ed7c164b9fa6068d82fab159ad\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=4321bdd657c21b22c10d0e12f493d4d808a85dec994ea06b5af48176f1450859\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mppv9sne/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","source":["import torch\n","from transformers import BertModel\n","from kobert_tokenizer import KoBERTTokenizer\n","import torch.nn as nn\n","\n","# KoBERT 모델 정의 (반드시 학습할 때와 동일한 구조여야 함)\n","class KoBERTNER(nn.Module):\n","    def __init__(self, num_labels):\n","        super(KoBERTNER, self).__init__()\n","        self.bert = BertModel.from_pretrained('skt/kobert-base-v1')\n","        self.dropout = nn.Dropout(0.3)\n","        self.classifier = nn.Linear(768, num_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n","            return loss, logits\n","        return logits\n","\n","# 태그 인덱스를 태그 이름으로 매핑 (반드시 학습할 때와 동일한 매핑이어야 함)\n","tag_to_id = {'B-ORG': 1, 'I-ORG': 2, 'B-MNY': 3, 'I-MNY': 4, 'B-PER': 5, 'I-PER': 6, 'O': 0}\n","id_to_tag = {v: k for k, v in tag_to_id.items()}\n","num_labels = len(tag_to_id)\n","\n","# 모델과 토크나이저 로드\n","MODEL_SAVE_PATH = \"/content/drive/MyDrive/프로젝트/증권 뉴스 분류 및 개체명 인식/kobert_ner_model.pth\"\n","TOKENIZER_SAVE_FOLDER = \"/content/drive/MyDrive/프로젝트/증권 뉴스 분류 및 개체명 인식/kobert_tokenizer_ner\"\n","max_len = 128\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = KoBERTNER(num_labels)\n","model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n","model.to(device)\n","model.eval()\n","\n","tokenizer = KoBERTTokenizer.from_pretrained(TOKENIZER_SAVE_FOLDER)"],"metadata":{"id":"FAJBNLh5sqCe","executionInfo":{"status":"ok","timestamp":1720434139641,"user_tz":-540,"elapsed":3526,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 음절 단위 토크나이저 함수 정의\n","def ner_tokenizer(sent, max_seq_length):\n","    pre_syllable = \"-\"\n","    input_ids = [0] * (max_seq_length - 1)\n","    attention_mask = [0] * (max_seq_length - 1)\n","    token_type_ids = [0] * max_seq_length\n","    sent = sent[:max_seq_length - 2]\n","\n","    for i, syllable in enumerate(sent):\n","        if syllable == '-':\n","            pre_syllable = syllable\n","        if pre_syllable != \"-\":\n","            syllable = '##' + syllable\n","        pre_syllable = syllable\n","\n","        input_ids[i] = tokenizer.convert_tokens_to_ids(syllable)\n","        attention_mask[i] = 1\n","\n","    input_ids = [2] + input_ids\n","    input_ids[len(sent) + 1] = 3\n","    attention_mask = [1] + attention_mask\n","    attention_mask[len(sent) + 1] = 1\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask,\n","        'token_type_ids': token_type_ids\n","    }"],"metadata":{"id":"_uvZGNT0sqAL","executionInfo":{"status":"ok","timestamp":1720434139641,"user_tz":-540,"elapsed":1,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 예측 함수 정의\n","def predict_sentence(model, tokenizer, sentence, tag_to_id, id_to_tag, max_len, device):\n","    model.eval()\n","\n","    # 음절 단위로 토큰화\n","    tokenized = ner_tokenizer(sentence, max_len)\n","    tokens = tokenized['input_ids']\n","    attention_mask = tokenized['attention_mask']\n","    token_type_ids = tokenized['token_type_ids']\n","\n","    # 데이터를 텐서로 변환\n","    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n","    attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0).to(device)\n","    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0).to(device)\n","\n","    # 모델 예측\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask, token_type_ids)\n","        predictions = torch.argmax(outputs, dim=2).cpu().numpy().flatten()\n","\n","    # 예측 결과 디코딩\n","    decoded_predictions = [id_to_tag[pred] for pred in predictions]\n","\n","    return decoded_predictions"],"metadata":{"id":"Og_DJxP4sp9S","executionInfo":{"status":"ok","timestamp":1720434141149,"user_tz":-540,"elapsed":1,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# 개체명 추출 함수 정의\n","def extract_entities(sentence, tags):\n","    entities = []\n","    entity = \"\"\n","    current_tag = None\n","\n","    for char, tag in zip(sentence, tags):\n","        if tag.startswith(\"B-\"):\n","            if entity:\n","                entities.append(entity)\n","            entity = char\n","            current_tag = tag[2:]\n","        elif tag.startswith(\"I-\") and current_tag == tag[2:]:\n","            entity += char\n","        else:\n","            if entity:\n","                entities.append(entity)\n","                entity = \"\"\n","            current_tag = None\n","\n","    if entity:\n","        entities.append(entity)\n","\n","    return entities"],"metadata":{"id":"2BGdT491s2Ji","executionInfo":{"status":"ok","timestamp":1720434144458,"user_tz":-540,"elapsed":404,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 테스트할 문장\n","input_sentence = \"삼성전자는 한국의 대표적인 전자 기업입니다.\"\n","\n","# 개체명 인식 수행\n","predicted_tags = predict_sentence(model, tokenizer, input_sentence, tag_to_id, id_to_tag, max_len, device)\n","\n","# 예측 결과에서 개체명 추출\n","entities = extract_entities(input_sentence, predicted_tags)\n","\n","# 예측 결과 출력\n","print(f\"Input Sentence: {input_sentence}\")\n","print(f\"Predicted Tags: {predicted_tags}\")\n","print(f\"Predicted Entities: {entities}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5WuO8d0s4kQ","executionInfo":{"status":"ok","timestamp":1720434147303,"user_tz":-540,"elapsed":1350,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"15a805b2-3080-4b02-9a5b-741525e20f89"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sentence: 삼성전자는 한국의 대표적인 전자 기업입니다.\n","Predicted Tags: ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","Predicted Entities: ['성전자']\n"]}]},{"cell_type":"code","source":["# 테스트할 문장\n","input_sentence = \"LG전자는 한국의 대표적인 전자 기업입니다.\"\n","\n","# 개체명 인식 수행\n","predicted_tags = predict_sentence(model, tokenizer, input_sentence, tag_to_id, id_to_tag, max_len, device)\n","\n","# 예측 결과에서 개체명 추출\n","entities = extract_entities(input_sentence, predicted_tags)\n","\n","# 예측 결과 출력\n","print(f\"Input Sentence: {input_sentence}\")\n","print(f\"Predicted Tags: {predicted_tags}\")\n","print(f\"Predicted Entities: {entities}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sonZwDah49E-","executionInfo":{"status":"ok","timestamp":1720434554654,"user_tz":-540,"elapsed":424,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"091e6950-98f1-47b4-a9e2-3c38f7293388"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sentence: LG전자는 한국의 대표적인 전자 기업입니다.\n","Predicted Tags: ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","Predicted Entities: ['G전자']\n"]}]}]}