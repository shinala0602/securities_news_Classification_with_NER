{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1rIcgOJxO2-BEXdATF4lUGH6zYmWLyOzN","authorship_tag":"ABX9TyMxI2J+YCPbBz5hlizRkjR3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuEbPkVjM0yz","executionInfo":{"status":"ok","timestamp":1720409404728,"user_tz":-540,"elapsed":44924,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"7ba47df7-5d99-490c-fcf1-56f7d3fa5b8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'NER'...\n","remote: Enumerating objects: 1770, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (42/42), done.\u001b[K\n","remote: Total 1770 (delta 0), reused 42 (delta 0), pack-reused 1727\u001b[K\n","Receiving objects: 100% (1770/1770), 20.69 MiB | 13.38 MiB/s, done.\n","Resolving deltas: 100% (146/146), done.\n","Updating files: 100% (1755/1755), done.\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-vlomiag4/kobert-tokenizer_4c1fb91b3aee49499019bba17b585201\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-vlomiag4/kobert-tokenizer_4c1fb91b3aee49499019bba17b585201\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=efcbe0587512f9cd01fe0c8e6f98722caf691ddd9a2cd81d91112250b84b18d3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-b4vak8_v/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}],"source":["!git clone https://github.com/kmounlp/NER.git\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI-iThtb3m87","executionInfo":{"status":"ok","timestamp":1720417429452,"user_tz":-540,"elapsed":10381,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"7563b0b9-968a-429a-8a4e-52f4dd47495d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=703f5aec0518632abee36aa7dc015e6bfe09d5f07d0df1efb24f2e68e81316eb\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","from pathlib import Path\n","import re\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","from transformers import BertModel, AdamW, get_cosine_schedule_with_warmup\n","from kobert_tokenizer import KoBERTTokenizer\n","import torch.nn as nn\n","from seqeval.metrics import accuracy_score, classification_report\n","\n","# 데이터 파일을 읽어와서 ORG, MNY, PET 태그만 살리고 나머지는 O로 처리하는 함수\n","def read_file(file_list):\n","    token_docs = []\n","    tag_docs = []\n","\n","    for file_path in file_list:\n","        file_path = Path(file_path)\n","        raw_text = file_path.read_text().strip()\n","        raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","\n","        for doc in raw_docs:\n","            tokens = []\n","            tags = []\n","            for line in doc.split('\\n'):\n","                if line.startswith((\"$\", \";\", \"##\")):\n","                    continue\n","                try:\n","                    token = line.split('\\t')[0]\n","                    tag = line.split('\\t')[3]\n","\n","                    # ORG, MNY, PET 태그 처리\n","                    if tag in ['B-ORG', 'I-ORG', 'B-MNY', 'I-MNY', 'B-PER', 'I-PER']:\n","                        pass\n","                    else:\n","                        tag = 'O'\n","\n","                    for i, syllable in enumerate(token):\n","                        tokens.append(syllable)\n","                        if i == 0:\n","                            tags.append(tag)\n","                        else:\n","                            modi_tag = 'I' + tag[1:] if tag.startswith('B') else tag\n","                            tags.append(modi_tag)\n","                except:\n","                    continue\n","            if len(tokens) == len(tags):\n","                token_docs.append(tokens)\n","                tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","# CSV 파일에서 회사명 데이터를 읽어와서 ORG 태그로 변환하는 함수\n","def read_company_names(file_path):\n","    df = pd.read_csv(file_path)\n","    company_names = df['회사명'].tolist()\n","\n","    token_docs = []\n","    tag_docs = []\n","\n","    for name in company_names:\n","        tokens = list(name.strip())\n","        tags = ['B-ORG'] + ['I-ORG'] * (len(tokens) - 1)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n","\n","# 실제 파일 경로로 설정\n","file_list = []\n","\n","for x in os.walk('/content/NER/말뭉치 - 형태소_개체명'):\n","    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):\n","        file_list.append(y)\n","\n","file_list = sorted(file_list)\n","\n","# 데이터 전처리\n","texts, tags = read_file(file_list)\n","\n","# CSV 파일에서 회사명 데이터 추가\n","company_texts, company_tags = read_company_names('/content/drive/MyDrive/프로젝트/증권 뉴스 분류 및 개체명 인식/상장법인목록.csv')\n","texts.extend(company_texts)\n","tags.extend(company_tags)\n","\n","# 데이터를 train과 test로 분할\n","train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=0.2, random_state=42)\n","\n","print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbK2c_hXfXXF","executionInfo":{"status":"ok","timestamp":1720428625796,"user_tz":-540,"elapsed":3015,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"f10663b6-cff2-4c80-8898-8949a65ded25"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Train samples: 17464, Test samples: 4367\n"]}]},{"cell_type":"code","source":["# 음절 단위 토크나이저 함수 정의\n","def ner_tokenizer(sent, max_seq_length):\n","    pre_syllable = \"-\"\n","    input_ids = [0] * (max_seq_length - 1)\n","    attention_mask = [0] * (max_seq_length - 1)\n","    token_type_ids = [0] * max_seq_length\n","    sent = sent[:max_seq_length - 2]\n","\n","    for i, syllable in enumerate(sent):\n","        if syllable == '-':\n","            pre_syllable = syllable\n","        if pre_syllable != \"-\":\n","            syllable = '##' + syllable\n","        pre_syllable = syllable\n","\n","        input_ids[i] = tokenizer.convert_tokens_to_ids(syllable)\n","        attention_mask[i] = 1\n","\n","    input_ids = [2] + input_ids\n","    input_ids[len(sent) + 1] = 3\n","    attention_mask = [1] + attention_mask\n","    attention_mask[len(sent) + 1] = 1\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask,\n","        'token_type_ids': token_type_ids\n","    }"],"metadata":{"id":"OuZvdfGoiLoh","executionInfo":{"status":"ok","timestamp":1720428628155,"user_tz":-540,"elapsed":356,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# 태그 인코딩 함수 정의\n","def encode_tags(tags, tag_to_id, max_seq_length):\n","    tags = tags[:max_seq_length - 2]\n","    labels = [tag_to_id[tag] for tag in tags]\n","    labels = [tag_to_id['O']] + labels + [tag_to_id['O']]\n","\n","    padding_length = max_seq_length - len(labels)\n","    labels += [tag_to_id['O']] * padding_length\n","\n","    return labels"],"metadata":{"id":"vCAND_mAiOI4","executionInfo":{"status":"ok","timestamp":1720428630446,"user_tz":-540,"elapsed":349,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 클래스 정의\n","class NERDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts, tags, tokenizer, tag_to_id, max_len):\n","        self.texts = texts\n","        self.tags = tags\n","        self.tokenizer = tokenizer\n","        self.tag_to_id = tag_to_id\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        tags = self.tags[idx]\n","\n","        # 음절 단위로 토큰화\n","        tokenized = ner_tokenizer(text, self.max_len)\n","        tokens = tokenized['input_ids']\n","        attention_mask = tokenized['attention_mask']\n","        token_type_ids = tokenized['token_type_ids']\n","\n","        labels = encode_tags(tags, self.tag_to_id, self.max_len)\n","\n","        return {\n","            'input_ids': torch.tensor(tokens, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(labels, dtype=torch.long)\n","        }"],"metadata":{"id":"9M6EiG04iQ2v","executionInfo":{"status":"ok","timestamp":1720428632046,"user_tz":-540,"elapsed":3,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# KoBERT 토크나이저 로드\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","\n","tag_to_id = {'B-ORG': 1, 'I-ORG': 2, 'B-MNY': 3, 'I-MNY': 4, 'B-PER': 5, 'I-PER': 6, 'O': 0}\n","max_len = 128\n","\n","train_dataset = NERDataset(train_texts, train_tags, tokenizer, tag_to_id, max_len)\n","test_dataset = NERDataset(test_texts, test_tags, tokenizer, tag_to_id, max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLKChPgUiULb","executionInfo":{"status":"ok","timestamp":1720428675607,"user_tz":-540,"elapsed":374,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"8e70287c-7bdb-4e60-991d-9f085652d4b0"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]}]},{"cell_type":"code","source":["def collate_fn(batch):\n","    input_ids = torch.stack([item['input_ids'] for item in batch])\n","    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n","    token_type_ids = torch.stack([item['token_type_ids'] for item in batch])\n","    labels = torch.stack([item['labels'] for item in batch])\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask,\n","        'token_type_ids': token_type_ids,\n","        'labels': labels\n","    }\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"],"metadata":{"id":"5-jbhpKgic3f","executionInfo":{"status":"ok","timestamp":1720428677213,"user_tz":-540,"elapsed":2,"user":{"displayName":"유인","userId":"09840046609030674956"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# 디버깅을 위해 첫 배치를 확인하는 코드 추\n","first_batch = next(iter(train_dataloader))\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4VaJLsuifnR","executionInfo":{"status":"ok","timestamp":1720428679527,"user_tz":-540,"elapsed":332,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"2863ac58-54db-4af6-e8e5-67c4d615916f"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[   2, 5561,    0,  ...,    0,    0,    0],\n","        [   2, 5907,    0,  ...,    0,    0,    0],\n","        [   2, 7005,    0,  ...,    0,    0,    0],\n","        ...,\n","        [   2, 7005,    0,  ...,    0,    0,    0],\n","        [   2,  285,    0,  ...,    0,    0,    0],\n","        [   2, 6521,    0,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 1, 2,  ..., 0, 0, 0],\n","        ...,\n","        [0, 1, 2,  ..., 0, 0, 0],\n","        [0, 1, 2,  ..., 0, 0, 0],\n","        [0, 1, 2,  ..., 0, 0, 0]])}\n"]}]},{"cell_type":"code","source":["# KoBERT 모델 정의\n","class KoBERTNER(nn.Module):\n","    def __init__(self, num_labels):\n","        super(KoBERTNER, self).__init__()\n","        self.bert = BertModel.from_pretrained('skt/kobert-base-v1')\n","        self.dropout = nn.Dropout(0.3)\n","        self.classifier = nn.Linear(768, num_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n","            return loss, logits\n","        return logits\n","\n","num_labels = len(tag_to_id)\n","model = KoBERTNER(num_labels)\n","\n","# GPU 설정\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","# 옵티마이저 및 스케줄러 설정\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","total_steps = len(train_dataloader) * 10\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","loss_function = nn.CrossEntropyLoss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khc9tTktiocq","executionInfo":{"status":"ok","timestamp":1720428731297,"user_tz":-540,"elapsed":1219,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"f26191d1-362d-4e6d-c571-06fa4c319404"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def train(epoch, model, dataloader, optimizer, scheduler, device, loss_function):\n","    model.train()\n","    for _, data in enumerate(dataloader, 0):\n","        ids = data['input_ids'].to(device, dtype=torch.long)\n","        mask = data['attention_mask'].to(device, dtype=torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","        targets = data['labels'].to(device, dtype=torch.long)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","        loss = loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n","\n","        if _ % 500 == 0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","for epoch in range(5):  # 원하는 epoch 수로 변경 가능\n","    train(epoch, model, train_dataloader, optimizer, scheduler, device, loss_function)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZWr0NO2ioaP","executionInfo":{"status":"ok","timestamp":1720430623234,"user_tz":-540,"elapsed":1881345,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"4071353b-ea59-4bf5-81cf-1039e9450364"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss:  1.969158411026001\n","Epoch: 0, Loss:  0.16629494726657867\n","Epoch: 0, Loss:  0.30614638328552246\n","Epoch: 1, Loss:  0.12371194362640381\n","Epoch: 1, Loss:  0.21896202862262726\n","Epoch: 1, Loss:  0.13116100430488586\n","Epoch: 2, Loss:  0.1541145145893097\n","Epoch: 2, Loss:  0.18381766974925995\n","Epoch: 2, Loss:  0.11312593519687653\n","Epoch: 3, Loss:  0.17277130484580994\n","Epoch: 3, Loss:  0.21344032883644104\n","Epoch: 3, Loss:  0.11048656702041626\n","Epoch: 4, Loss:  0.10978903621435165\n","Epoch: 4, Loss:  0.1971770077943802\n","Epoch: 4, Loss:  0.1985599547624588\n"]}]},{"cell_type":"code","source":["def validate(model, dataloader, device):\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    with torch.no_grad():\n","        for _, data in enumerate(dataloader, 0):\n","            ids = data['input_ids'].to(device, dtype=torch.long)\n","            mask = data['attention_mask'].to(device, dtype=torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['labels'].to(device, dtype=torch.long)\n","\n","            outputs = model(ids, mask, token_type_ids)\n","            fin_outputs.extend(torch.argmax(outputs, dim=2).cpu().numpy().flatten())\n","            fin_targets.extend(targets.cpu().numpy().flatten())\n","    return fin_outputs, fin_targets\n","\n","outputs, targets = validate(model, test_dataloader, device)\n","accuracy = accuracy_score(targets, outputs)\n","print(f'Validation Accuracy: {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJzXF1I-ioYG","executionInfo":{"status":"ok","timestamp":1720430774675,"user_tz":-540,"elapsed":31240,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"6ad147d2-cb62-4f10-cd3e-918f0457b598"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.9644152879551179\n"]}]},{"cell_type":"code","source":["def predict_sentence(model, tokenizer, sentence, tag_to_id, id_to_tag, max_len, device):\n","    model.eval()\n","\n","    # 음절 단위로 토큰화\n","    tokenized = ner_tokenizer(sentence, max_len)\n","    tokens = tokenized['input_ids']\n","    attention_mask = tokenized['attention_mask']\n","    token_type_ids = tokenized['token_type_ids']\n","\n","    # 데이터를 텐서로 변환\n","    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n","    attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0).to(device)\n","    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0).to(device)\n","\n","    # 모델 예측\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask, token_type_ids)\n","        predictions = torch.argmax(outputs, dim=2).cpu().numpy().flatten()\n","\n","    # 예측 결과 디코딩\n","    decoded_predictions = [id_to_tag[pred] for pred in predictions]\n","\n","    return decoded_predictions\n","\n","def extract_entities(sentence, tags):\n","    entities = []\n","    entity = \"\"\n","    current_tag = None\n","\n","    for char, tag in zip(sentence, tags):\n","        if tag.startswith(\"B-\"):\n","            if entity:\n","                entities.append(entity)\n","            entity = char\n","            current_tag = tag[2:]\n","        elif tag.startswith(\"I-\") and current_tag == tag[2:]:\n","            entity += char\n","        else:\n","            if entity:\n","                entities.append(entity)\n","                entity = \"\"\n","            current_tag = None\n","\n","    if entity:\n","        entities.append(entity)\n","\n","    return entities\n","\n","# 테스트할 문장\n","input_sentence = \"이재용\"\n","\n","# 개체명 인식 수행\n","id_to_tag = {v: k for k, v in tag_to_id.items()}  # 태그 인덱스를 태그 이름으로 매핑\n","predicted_tags = predict_sentence(model, tokenizer, input_sentence, tag_to_id, id_to_tag, max_len, device)\n","\n","# 예측 결과에서 개체명 추출\n","entities = extract_entities(input_sentence, predicted_tags)\n","\n","# 예측 결과 출력\n","print(f\"Input Sentence: {input_sentence}\")\n","print(f\"Predicted Entities: {entities}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LsOpbywkDy_","executionInfo":{"status":"ok","timestamp":1720431128823,"user_tz":-540,"elapsed":435,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"4cbcd0d7-23f9-47a4-a05f-25e6d3ea550c"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sentence: 이재용\n","Predicted Entities: ['재용']\n"]}]},{"cell_type":"code","source":["# 모델과 토크나이저 저장\n","MODEL_SAVE_PATH = \"kobert_ner_model.pth\"\n","TOKENIZER_SAVE_FOLDER = \"kobert_tokenizer_ner\"\n","\n","if not os.path.exists(TOKENIZER_SAVE_FOLDER):\n","    os.makedirs(TOKENIZER_SAVE_FOLDER)\n","\n","torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","tokenizer.save_pretrained(TOKENIZER_SAVE_FOLDER)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHyuUgtci6a4","executionInfo":{"status":"ok","timestamp":1720431215889,"user_tz":-540,"elapsed":1999,"user":{"displayName":"유인","userId":"09840046609030674956"}},"outputId":"6fdeb836-bf0e-4c5d-cc75-c1253c734a4a"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('kobert_tokenizer_ner/tokenizer_config.json',\n"," 'kobert_tokenizer_ner/special_tokens_map.json',\n"," 'kobert_tokenizer_ner/spiece.model',\n"," 'kobert_tokenizer_ner/added_tokens.json')"]},"metadata":{},"execution_count":85}]}]}